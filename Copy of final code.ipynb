{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"final code.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MSeeFnuKgFwB"},"source":["# Download Dataset\n"]},{"cell_type":"code","metadata":{"id":"IZ29GTsPgN_6"},"source":["# ALL_OUTPUT =   a list of input sentences\n","# ALL_INPUT =  a list of output labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ZBuMU_Ufokc"},"source":["# Hyper Parameters"]},{"cell_type":"code","metadata":{"id":"TqmrmcA_e4P4"},"source":["WORD_EMBEDDING = 0     # 0=word2vec, 1=fasttext, 2=glove\n","DIM = 25               # number of streams\n","D2D_THRESHOLD = 15\n","POOLING = \"max\"        # \"max\",\"min\",\"avg\"\n","\n","ALL_USED = False\n","USED_SIZE = 3000\n","TRAIN_PORTION = 0.01\n","\n","HIDDEN_DIM = 25\n","DROP_OUT = 0.5\n","LR = 0.002\n","WEIGHT_DECAY =  0\n","EPOCH = 2000\n","EARLY_STOPPING = 100\n","\n","VAL_PORTION = 0.1\n","REMOVE_LESS_FREQUENT = 5\n","NUM_TEST = 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IWXgdO1_bl9a"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"id":"rF4UCFigbljI"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","from tqdm import tqdm\n","import time\n","\n","import math\n","from math import log\n","import scipy.sparse as sp\n","\n","import nltk\n","from nltk.corpus import stopwords\n","\n","from gensim.models import Word2Vec\n","from gensim.models import FastText\n","# from glove import Corpus, Glove\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","\n","import torch\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fNID2f6oglDj"},"source":["# Preprocess"]},{"cell_type":"markdown","metadata":{"id":"RcJWvIASgm-h"},"source":["## Train Test Split"]},{"cell_type":"code","metadata":{"id":"s-cf0TVugmVy"},"source":["if ALL_USED:\n","    sent_used, label_used = ALL_INPUT, ALL_OUTPUT\n","else:\n","    sent_used,_, label_used, _ = train_test_split(ALL_INPUT ,ALL_OUTPUT, train_size = USED_SIZE, stratify = ALL_OUTPUT, random_state = 0 )\n","\n","not_all = False\n","try:\n","    train_sent, test_sent, train_labels, test_labels = train_test_split(sent_used ,label_used, stratify = label_used, train_size = TRAIN_PORTION, random_state = 0 ) \n","except:\n","    train_sent, test_sent, train_labels, test_labels = train_test_split(sent_used ,label_used, train_size = TRAIN_PORTION, random_state = 0 ) \n","\n","unique_train = np.unique(train_labels)\n","unique_test = np.unique(test_labels)\n","for label in unique_test:\n","    if label not in unique_train:\n","        not_all = True        \n","        break\n","\n","if not_all:\n","    labels_to_add = [label for label in unique_test if label not in unique_train]\n","    label_add_set = set(labels_to_add)\n","    i = 0\n","    while len(label_add_set)>0:\n","        label = test_labels[i]\n","        if label in label_add_set:\n","            train_sent.append(test_sent[i])\n","            train_labels.append(test_labels[i])\n","            test_sent = test_sent[:i]+test_sent[i+1:]\n","            test_labels = test_labels[:i]+test_labels[i+1:]\n","            label_add_set.remove(label)\n","        else:\n","            i += 1\n","\n","original_sentences = train_sent+test_sent\n","train_size = len(train_sent)\n","test_size = len(test_sent)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9M9VSoEhkFpc"},"source":["## Label Encoding"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ov3cmsx3kHVm","executionInfo":{"status":"ok","timestamp":1612248640977,"user_tz":-660,"elapsed":41985,"user":{"displayName":"Kunze Wang","photoUrl":"","userId":"02249298630517649291"}},"outputId":"17161756-3288-41ac-f060-9b7a24ebd538"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","unique_labels=np.unique(train_labels + test_labels)\n","\n","num_class = len(unique_labels)\n","lEnc = LabelEncoder()\n","lEnc.fit(unique_labels)\n","\n","print(unique_labels)\n","print(lEnc.transform(unique_labels))\n","\n","train_labels = lEnc.transform(train_labels)\n","test_labels = lEnc.transform(test_labels)\n","labels = train_labels.tolist()+test_labels.tolist()\n","labels = torch.LongTensor(labels).to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['acq' 'alum' 'bop' 'carcass' 'cocoa' 'coffee' 'copper' 'cotton' 'cpi'\n"," 'cpu' 'crude' 'dlr' 'earn' 'fuel' 'gas' 'gnp' 'gold' 'grain' 'heat'\n"," 'housing' 'income' 'instal-debt' 'interest' 'ipi' 'iron-steel' 'jet'\n"," 'jobs' 'lead' 'lei' 'livestock' 'lumber' 'meal-feed' 'money-fx'\n"," 'money-supply' 'nat-gas' 'nickel' 'orange' 'pet-chem' 'platinum' 'potato'\n"," 'reserves' 'retail' 'rubber' 'ship' 'strategic-metal' 'sugar' 'tea' 'tin'\n"," 'trade' 'veg-oil' 'wpi' 'zinc']\n","[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n"," 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n"," 48 49 50 51]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bJWt8UNQkUuc"},"source":["## Remove Stopwords and less frequent words, tokenize sentences"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xk8eDovGkZgq","executionInfo":{"status":"ok","timestamp":1612248641414,"user_tz":-660,"elapsed":42420,"user":{"displayName":"Kunze Wang","photoUrl":"","userId":"02249298630517649291"}},"outputId":"ddda0837-f0ea-4954-c11e-daf82d30977a"},"source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","def clean_str(string):\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" \\( \", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    return string.strip().lower()\n","\n","original_word_freq = {}  # to remove rare words\n","for sentence in original_sentences:\n","    temp = clean_str(sentence)\n","    word_list = temp.split()\n","    for word in word_list:\n","        if word in original_word_freq:\n","            original_word_freq[word] += 1\n","        else:\n","            original_word_freq[word] = 1   \n","\n","tokenize_sentences = []\n","word_list_dict = {}\n","for sentence in original_sentences:\n","    temp = clean_str(sentence)\n","    word_list_temp = temp.split()\n","    doc_words = []\n","    for word in word_list_temp:\n","        if word not in stop_words and original_word_freq[word] >= REMOVE_LESS_FREQUENT:\n","            doc_words.append(word)\n","            word_list_dict[word] = 1\n","    tokenize_sentences.append(doc_words)\n","word_list = list(word_list_dict.keys())\n","vocab_length = len(word_list)\n","\n","del original_sentences\n","\n","#word to id dict\n","word_id_map = {}\n","for i in range(vocab_length):\n","    word_id_map[word_list[i]] = i           "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bhNMufnXt58Q"},"source":["## W2V"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pds9v4yVt-zP","executionInfo":{"status":"ok","timestamp":1612248675638,"user_tz":-660,"elapsed":76641,"user":{"displayName":"Kunze Wang","photoUrl":"","userId":"02249298630517649291"}},"outputId":"ba01ccbb-c019-4c54-9fee-c20a98ecfbe2"},"source":["if WORD_EMBEDDING == 0:\n","    wv_cbow_model = Word2Vec(sentences=tokenize_sentences, size=DIM, window=5, min_count=0, workers=4, sg=0, iter=200)\n","    word_emb_dict = {}\n","    for word in word_list:\n","        word_emb_dict[word] = wv_cbow_model[word].tolist()\n","elif WORD_EMBEDDING == 1:\n","    ft_sg_model = FastText(sentences=tokenize_sentences, size=DIM, window=5, min_count=0, workers=4, sg=0, iter = 200)\n","    word_emb_dict = {}\n","    for word in word_list:\n","        word_emb_dict[word] = ft_sg_model[word].tolist()\n","elif WORD_EMBEDDING == 2:\n","\n","    corpus = Corpus() \n","    corpus.fit(tokenize_sentences, window=10)\n","\n","    glove = Glove(no_components=DIM, learning_rate=0.05) \n","    glove.fit(corpus.matrix, epochs=200, no_threads=4, verbose=True)\n","    glove.add_dictionary(corpus.dictionary)\n","\n","    word_emb_dict = {}\n","    for word in word_list:\n","        word_emb_dict[word] = glove.word_vectors[glove.dictionary[word]].tolist()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"mxSc7knct_W9"},"source":["## Doc2vec"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CsfLI8_uuAtM","executionInfo":{"status":"ok","timestamp":1612248763506,"user_tz":-660,"elapsed":164504,"user":{"displayName":"Kunze Wang","photoUrl":"","userId":"02249298630517649291"}},"outputId":"8f7fa2b1-1587-4497-ef07-e133a87d9d38"},"source":["documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenize_sentences)]\n","model = Doc2Vec(documents, vector_size=DIM, window=5, min_count=1, workers=4, iter=200)\n","\n","doc2vec_emb = []\n","for i in range(len(documents)):\n","    doc2vec_emb.append(model.docvecs[i])\n","doc2vec_npy = np.array(doc2vec_emb)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py:566: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n","  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"p51GGz5muEiH"},"source":["# Graph"]},{"cell_type":"code","metadata":{"id":"DxHkXfzbuFi_"},"source":["\n","\n","node_size = train_size + vocab_length + test_size\n","adj_tensor = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2DRCSb_uQ3P"},"source":["## d2w: tfidf"]},{"cell_type":"code","metadata":{"id":"wKIbVTKZuTsz"},"source":["tfidf_row = []\n","tfidf_col = []\n","tfidf_weight = []\n","\n","#get each word appears in which document\n","word_doc_list = {}\n","for word in word_list:\n","    word_doc_list[word]=[]\n","\n","for i in range(len(tokenize_sentences)):\n","    doc_words = tokenize_sentences[i]\n","    unique_words = set(doc_words)\n","    for word in unique_words:\n","        exsit_list = word_doc_list[word]\n","        exsit_list.append(i)\n","        word_doc_list[word] = exsit_list\n","\n","#document frequency\n","word_doc_freq = {}\n","for word, doc_list in word_doc_list.items():\n","    word_doc_freq[word] = len(doc_list)\n","\n","# term frequency\n","doc_word_freq = {}\n","\n","for doc_id in range(len(tokenize_sentences)):\n","    words = tokenize_sentences[doc_id]\n","    for word in words:\n","        word_id = word_id_map[word]\n","        doc_word_str = str(doc_id) + ',' + str(word_id)\n","        if doc_word_str in doc_word_freq:\n","            doc_word_freq[doc_word_str] += 1\n","        else:\n","            doc_word_freq[doc_word_str] = 1\n","\n","for i in range(len(tokenize_sentences)):\n","    words = tokenize_sentences[i]\n","    doc_word_set = set()\n","    for word in words:\n","        if word in doc_word_set:\n","            continue\n","        j = word_id_map[word]\n","        key = str(i) + ',' + str(j)\n","        freq = doc_word_freq[key]\n","        if i < train_size:\n","            row_tmp = i\n","        else:\n","            row_tmp = i + vocab_length\n","        col_tmp = train_size + j\n","        \n","        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n","        weight_tmp = freq * idf\n","        doc_word_set.add(word)\n","\n","        tfidf_row.append(row_tmp)\n","        tfidf_col.append(col_tmp)\n","        tfidf_weight.append(weight_tmp)\n","\n","        tfidf_row.append(col_tmp)\n","        tfidf_col.append(row_tmp)\n","        tfidf_weight.append(weight_tmp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBuUG6PhuZ3G"},"source":["## Diagonal"]},{"cell_type":"code","metadata":{"id":"uRsUi8DluZU2"},"source":["for i in range(node_size):\n","    tfidf_row.append(i)\n","    tfidf_col.append(i)\n","    tfidf_weight.append(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bb0NaYnIudwg"},"source":["## w2w and d2d"]},{"cell_type":"code","metadata":{"id":"kj5YrPgMuisr"},"source":["def ordered_word_pair(a, b):\n","  if a > b:\n","    return (b, a)\n","  else:\n","    return (a, b)\n","\n","co_dict = {}\n","for sent in tokenize_sentences:\n","    for i,word1 in enumerate(sent):\n","        for word2 in sent[i:]:\n","            co_dict[ordered_word_pair(word_id_map[word1],word_id_map[word2])] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26vo-FELulrx"},"source":["co_occur_threshold = D2D_THRESHOLD\n","\n","doc_vec_bow = []\n","for sent in tokenize_sentences:\n","    temp = np.zeros((vocab_length))\n","    for word in sent:\n","        temp[word_id_map[word]] = 1\n","    doc_vec_bow.append(temp)\n","\n","co_doc_dict = {}\n","for i in range(len(doc_vec_bow)-1):\n","    for j in range(i+1,len(doc_vec_bow)):\n","        if np.dot(doc_vec_bow[i],doc_vec_bow[j]) >= co_occur_threshold:\n","            co_doc_dict[(i,j)] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4wEzXINutVW"},"source":["def normalize_adj(adj):\n","    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n","    adj = sp.coo_matrix(adj)\n","    rowsum = np.array(adj.sum(1))\n","    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n","    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n","    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n","    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n","\n","def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(\n","        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzs_wtbUuvu8","executionInfo":{"status":"ok","timestamp":1612249051279,"user_tz":-660,"elapsed":452250,"user":{"displayName":"Kunze Wang","photoUrl":"","userId":"02249298630517649291"}},"outputId":"3a88e693-f126-4d26-c2d6-f9e67d735db8"},"source":["adj_list = []\n","\n","for i in tqdm(range(DIM)):\n","    col = tfidf_col[:]\n","    row = tfidf_row[:]\n","    weight = tfidf_weight[:]\n","    for pair in co_dict:\n","        ind1, ind2 = pair\n","\n","        word1 = word_list[ind1]\n","        word2 = word_list[ind2]\n","        tmp = np.tanh(1/np.abs(word_emb_dict[word1][i] - word_emb_dict[word2][i]))\n","\n","        row.append(ind2+train_size)\n","        col.append(ind1+train_size)\n","        weight.append(tmp)\n","\n","        row.append(ind1+train_size)\n","        col.append(ind2+train_size)\n","        weight.append(tmp)\n","\n","    for pair in co_doc_dict:\n","        ind1, ind2 = pair        \n","        tmp = np.tanh(1/np.abs(doc2vec_npy[ind1][i] - doc2vec_npy[ind2][i]))\n","\n","        if ind1>train_size:\n","            ind1 += vocab_length\n","        if ind2>train_size:    \n","            ind2 += vocab_length\n","\n","        row.append(ind2)\n","        col.append(ind1)\n","        weight.append(tmp)\n","\n","        row.append(ind1)\n","        col.append(ind2)\n","        weight.append(tmp)    \n","\n","    \n","    adj_tmp = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n","    adj_tmp = adj_tmp + adj_tmp.T.multiply(adj_tmp.T > adj_tmp) - adj_tmp.multiply(adj_tmp.T > adj_tmp)\n","    adj_tmp = normalize_adj(adj_tmp) \n","    adj_tmp = sparse_mx_to_torch_sparse_tensor(adj_tmp)\n","    adj_list.append(adj_tmp)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/25 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n","  4%|▍         | 1/25 [00:10<04:07, 10.30s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n","  8%|▊         | 2/25 [00:20<03:56, 10.29s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 12%|█▏        | 3/25 [00:30<03:45, 10.27s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 16%|█▌        | 4/25 [00:41<03:36, 10.31s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 20%|██        | 5/25 [00:51<03:25, 10.30s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 24%|██▍       | 6/25 [01:01<03:15, 10.27s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 28%|██▊       | 7/25 [01:11<03:04, 10.24s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 32%|███▏      | 8/25 [01:21<02:53, 10.21s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 36%|███▌      | 9/25 [01:32<02:43, 10.20s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 40%|████      | 10/25 [01:42<02:32, 10.18s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 44%|████▍     | 11/25 [01:52<02:22, 10.18s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 48%|████▊     | 12/25 [02:02<02:12, 10.17s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 52%|█████▏    | 13/25 [02:12<02:02, 10.17s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 56%|█████▌    | 14/25 [02:23<01:52, 10.20s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 60%|██████    | 15/25 [02:33<01:41, 10.20s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 64%|██████▍   | 16/25 [02:43<01:31, 10.19s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 68%|██████▊   | 17/25 [02:53<01:21, 10.19s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 72%|███████▏  | 18/25 [03:03<01:11, 10.22s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 76%|███████▌  | 19/25 [03:14<01:01, 10.20s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 80%|████████  | 20/25 [03:24<00:50, 10.18s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 84%|████████▍ | 21/25 [03:34<00:40, 10.17s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 88%|████████▊ | 22/25 [03:44<00:30, 10.17s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 92%|█████████▏| 23/25 [03:54<00:20, 10.19s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n"," 96%|█████████▌| 24/25 [04:04<00:10, 10.18s/it]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n","  if sys.path[0] == '':\n","100%|██████████| 25/25 [04:15<00:00, 10.20s/it]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"TL2fUXKXMWb1"},"source":["# Model - MULTIGCN"]},{"cell_type":"markdown","metadata":{"id":"zHAwQ20NM7b4"},"source":["## input features - glove and doc2vec"]},{"cell_type":"code","metadata":{"id":"8Ja1YCR0M6vl"},"source":["features = []\n","for i in range(train_size):\n","    features.append(doc2vec_npy[i])\n","\n","for word in word_list:\n","    features.append(word_emb_dict[word])\n","\n","for i in range(test_size):\n","    features.append(doc2vec_npy[train_size+i])\n","\n","features = torch.FloatTensor(np.array(features)).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WMIkzc3VM-9m"},"source":["## GCN layer"]},{"cell_type":"code","metadata":{"id":"-fkfj_38MaAi"},"source":["class GraphConvolution(Module):\n","    \"\"\"\n","    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n","        super(GraphConvolution, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n","        if bias:\n","            self.bias = Parameter(torch.zeros(1, out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters(in_features, out_features)\n","        self.dropout = torch.nn.Dropout(drop_out)\n","        self.activation =  activation\n","\n","    def reset_parameters(self,in_features, out_features):\n","        stdv = np.sqrt(6.0/(in_features+out_features))\n","        # stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv)\n","        # if self.bias is not None:\n","        #     torch.nn.init.zeros_(self.bias)\n","            # self.bias.data.uniform_(-stdv, stdv)\n","\n","\n","    def forward(self, input, adj, feature_less=False):\n","        if feature_less:\n","            support = self.weight\n","        else:\n","            input = self.dropout(input)\n","            support = torch.mm(input, self.weight)\n","        output = torch.spmm(adj, support)\n","        if self.bias is not None:\n","            output = output + self.bias\n","        if self.activation is not None:\n","            output = self.activation(output)\n","        return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' \\\n","               + str(self.in_features) + ' -> ' \\\n","               + str(self.out_features) + ')'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqi1SA7UNlmS"},"source":["## Main Model"]},{"cell_type":"code","metadata":{"id":"6J7QWgDmNm68"},"source":["class MULTIGCN(nn.Module):\n","    def __init__(self, nfeat, nhid, nclass, dropout):\n","        super(MULTIGCN, self).__init__()\n","\n","        # different weights\n","        self.intras1 = nn.ModuleList([GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU()) for i in range(DIM)])\n","        self.intras2 = nn.ModuleList([GraphConvolution(nhid*DIM, nclass, dropout, activation = nn.ReLU()) for i in range(DIM)])\n","\n","\n","    def forward(self, x, adj, feature_less=False):\n","        x = torch.stack([self.intras1[i](x,adj[i],feature_less) for i in range(DIM)]) \n","        x = x.permute(1,0,2) \n","        x = x.reshape(x.size()[0],-1)  \n","        x = torch.stack([self.intras2[i](x,adj[i]) for i in range(DIM)]) \n","\n"," \n","        if POOLING == 'avg':\n","            return torch.mean(x,0)\n","        if POOLING == 'max':\n","            return torch.max(x,0)[0]\n","        if POOLING == 'min':\n","            return torch.min(x,0)[0]   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CcalkQZdRNMU"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"_2KLaZBznhB6"},"source":["real_train_size = int((1-VAL_PORTION)*train_size)\n","val_size = train_size-real_train_size\n","\n","idx_train = range(real_train_size)\n","idx_val = range(real_train_size,train_size)\n","idx_test = range(train_size + vocab_length,node_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ArKytc7CRPJE","executionInfo":{"status":"ok","timestamp":1612249744039,"user_tz":-660,"elapsed":508073,"user":{"displayName":"Kunze Wang","photoUrl":"","userId":"02249298630517649291"}},"outputId":"6e564ce7-9dc6-4256-f951-60c33f644f58"},"source":["# Model and optimizer\n","\n","def cal_accuracy(predictions,labels):\n","    pred = torch.argmax(predictions,-1).cpu().tolist()\n","    lab = labels.cpu().tolist()\n","    cor = 0\n","    for i in range(len(pred)):\n","        if pred[i] == lab[i]:\n","            cor += 1\n","    return cor/len(pred)\n","\n","\n","final_acc_list = []\n","for _ in range(NUM_TEST):\n","    model = MULTIGCN(nfeat=features.shape[1], nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    val_loss = []\n","    for epoch in range(EPOCH):\n","\n","        t = time.time()\n","        model.train()\n","        optimizer.zero_grad()\n","        output = model(features, adj_list)\n","        loss_train = criterion(output[idx_train], labels[idx_train])\n","        acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n","        loss_train.backward()\n","        optimizer.step()\n","\n","\n","        model.eval()\n","        output = model(features, adj_list)\n","\n","        loss_val = criterion(output[idx_val], labels[idx_val])\n","        val_loss.append(loss_val.item())\n","        acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n","        print(  'Epoch: {:04d}'.format(epoch+1),\n","                'loss_train: {:.4f}'.format(loss_train.item()),\n","                'acc_train: {:.4f}'.format(acc_train),\n","                'loss_val: {:.4f}'.format(loss_val.item()),\n","                'acc_val: {:.4f}'.format(acc_val),\n","                'time: {:.4f}s'.format(time.time() - t))\n","        \n","        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n","            print(\"Early Stopping...\")\n","            break\n","\n","    model.eval()\n","    output = model(features, adj_list)\n","    loss_test = criterion(output[idx_test], labels[-test_size:])\n","    acc_test = cal_accuracy(output[idx_test], labels[-test_size:])\n","    print(\"Test set results:\",\n","            \"loss= {:.4f}\".format(loss_test.item()),\n","            \"accuracy= {:.4f}\".format(acc_test))\n","\n","    final_acc_list.append(acc_test)\n","\n","    print(classification_report(test_labels,torch.argmax(output[idx_test],-1).cpu().tolist(),digits = 4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0001 loss_train: 3.9576 acc_train: 0.0299 loss_val: 4.1145 acc_val: 0.0000 time: 1.1984s\n","Epoch: 0002 loss_train: 3.6808 acc_train: 0.1940 loss_val: 4.3477 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0003 loss_train: 3.3607 acc_train: 0.3284 loss_val: 4.5684 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0004 loss_train: 3.1862 acc_train: 0.2985 loss_val: 4.6943 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0005 loss_train: 3.0532 acc_train: 0.2388 loss_val: 4.7881 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0006 loss_train: 2.9050 acc_train: 0.2537 loss_val: 4.8313 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0007 loss_train: 2.7349 acc_train: 0.2985 loss_val: 4.8818 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0008 loss_train: 2.6715 acc_train: 0.2687 loss_val: 4.9247 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0009 loss_train: 2.5774 acc_train: 0.3134 loss_val: 4.9594 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0010 loss_train: 2.5405 acc_train: 0.3134 loss_val: 4.9911 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0011 loss_train: 2.4656 acc_train: 0.2687 loss_val: 5.0191 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0012 loss_train: 2.3591 acc_train: 0.3582 loss_val: 5.0452 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0013 loss_train: 2.2278 acc_train: 0.4328 loss_val: 5.0680 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0014 loss_train: 2.1319 acc_train: 0.4328 loss_val: 5.0910 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0015 loss_train: 2.0401 acc_train: 0.5373 loss_val: 5.1167 acc_val: 0.0000 time: 0.9908s\n","Epoch: 0016 loss_train: 1.9877 acc_train: 0.5821 loss_val: 5.1480 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0017 loss_train: 1.9055 acc_train: 0.5970 loss_val: 5.1829 acc_val: 0.0000 time: 0.9879s\n","Epoch: 0018 loss_train: 1.7825 acc_train: 0.7015 loss_val: 5.2171 acc_val: 0.0000 time: 0.9915s\n","Epoch: 0019 loss_train: 1.7778 acc_train: 0.6567 loss_val: 5.2509 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0020 loss_train: 1.6364 acc_train: 0.8060 loss_val: 5.2838 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0021 loss_train: 1.5746 acc_train: 0.7761 loss_val: 5.3176 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0022 loss_train: 1.5233 acc_train: 0.7761 loss_val: 5.3499 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0023 loss_train: 1.4833 acc_train: 0.7612 loss_val: 5.3800 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0024 loss_train: 1.4205 acc_train: 0.8507 loss_val: 5.4088 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0025 loss_train: 1.3447 acc_train: 0.8657 loss_val: 5.4374 acc_val: 0.0000 time: 0.9874s\n","Epoch: 0026 loss_train: 1.3108 acc_train: 0.8507 loss_val: 5.4640 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0027 loss_train: 1.2407 acc_train: 0.8806 loss_val: 5.4903 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0028 loss_train: 1.2147 acc_train: 0.8358 loss_val: 5.5188 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0029 loss_train: 1.1750 acc_train: 0.8955 loss_val: 5.5485 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0030 loss_train: 1.0917 acc_train: 0.8955 loss_val: 5.5790 acc_val: 0.0000 time: 0.9880s\n","Epoch: 0031 loss_train: 1.1084 acc_train: 0.8955 loss_val: 5.6085 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0032 loss_train: 1.0118 acc_train: 0.8955 loss_val: 5.6425 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0033 loss_train: 0.9782 acc_train: 0.9403 loss_val: 5.6787 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0034 loss_train: 0.9401 acc_train: 0.9254 loss_val: 5.7150 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0035 loss_train: 0.9134 acc_train: 0.9552 loss_val: 5.7531 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0036 loss_train: 0.8293 acc_train: 0.9552 loss_val: 5.7909 acc_val: 0.0000 time: 0.9873s\n","Epoch: 0037 loss_train: 0.8111 acc_train: 0.9701 loss_val: 5.8300 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0038 loss_train: 0.7467 acc_train: 0.9552 loss_val: 5.8685 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0039 loss_train: 0.7483 acc_train: 0.9552 loss_val: 5.9058 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0040 loss_train: 0.7092 acc_train: 0.9701 loss_val: 5.9414 acc_val: 0.0000 time: 0.9866s\n","Epoch: 0041 loss_train: 0.6959 acc_train: 0.9701 loss_val: 5.9702 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0042 loss_train: 0.6635 acc_train: 0.9701 loss_val: 6.0013 acc_val: 0.0000 time: 0.9873s\n","Epoch: 0043 loss_train: 0.6456 acc_train: 0.9701 loss_val: 6.0323 acc_val: 0.0000 time: 0.9870s\n","Epoch: 0044 loss_train: 0.5979 acc_train: 1.0000 loss_val: 6.0656 acc_val: 0.0000 time: 0.9871s\n","Epoch: 0045 loss_train: 0.6025 acc_train: 0.9403 loss_val: 6.0969 acc_val: 0.0000 time: 0.9872s\n","Epoch: 0046 loss_train: 0.5389 acc_train: 0.9851 loss_val: 6.1263 acc_val: 0.0000 time: 0.9877s\n","Epoch: 0047 loss_train: 0.5606 acc_train: 0.9701 loss_val: 6.1549 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0048 loss_train: 0.5111 acc_train: 0.9851 loss_val: 6.1806 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0049 loss_train: 0.4805 acc_train: 1.0000 loss_val: 6.2073 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0050 loss_train: 0.4559 acc_train: 0.9851 loss_val: 6.2343 acc_val: 0.0000 time: 0.9878s\n","Epoch: 0051 loss_train: 0.4477 acc_train: 1.0000 loss_val: 6.2659 acc_val: 0.0000 time: 0.9871s\n","Epoch: 0052 loss_train: 0.4250 acc_train: 0.9851 loss_val: 6.3031 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0053 loss_train: 0.4444 acc_train: 0.9701 loss_val: 6.3384 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0054 loss_train: 0.4002 acc_train: 1.0000 loss_val: 6.3739 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0055 loss_train: 0.3938 acc_train: 0.9851 loss_val: 6.4039 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0056 loss_train: 0.3763 acc_train: 1.0000 loss_val: 6.4347 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0057 loss_train: 0.3745 acc_train: 0.9851 loss_val: 6.4647 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0058 loss_train: 0.3512 acc_train: 0.9701 loss_val: 6.4987 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0059 loss_train: 0.3214 acc_train: 1.0000 loss_val: 6.5372 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0060 loss_train: 0.3224 acc_train: 1.0000 loss_val: 6.5732 acc_val: 0.0000 time: 0.9860s\n","Epoch: 0061 loss_train: 0.3274 acc_train: 1.0000 loss_val: 6.6077 acc_val: 0.0000 time: 0.9880s\n","Epoch: 0062 loss_train: 0.3070 acc_train: 1.0000 loss_val: 6.6389 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0063 loss_train: 0.2742 acc_train: 1.0000 loss_val: 6.6702 acc_val: 0.0000 time: 0.9874s\n","Epoch: 0064 loss_train: 0.3076 acc_train: 1.0000 loss_val: 6.6981 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0065 loss_train: 0.2713 acc_train: 1.0000 loss_val: 6.7257 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0066 loss_train: 0.2742 acc_train: 1.0000 loss_val: 6.7530 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0067 loss_train: 0.2700 acc_train: 1.0000 loss_val: 6.7841 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0068 loss_train: 0.2464 acc_train: 1.0000 loss_val: 6.8146 acc_val: 0.0000 time: 0.9879s\n","Epoch: 0069 loss_train: 0.2315 acc_train: 1.0000 loss_val: 6.8456 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0070 loss_train: 0.2433 acc_train: 1.0000 loss_val: 6.8755 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0071 loss_train: 0.2579 acc_train: 0.9851 loss_val: 6.9021 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0072 loss_train: 0.2237 acc_train: 1.0000 loss_val: 6.9225 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0073 loss_train: 0.2104 acc_train: 1.0000 loss_val: 6.9358 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0074 loss_train: 0.1839 acc_train: 1.0000 loss_val: 6.9497 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0075 loss_train: 0.2018 acc_train: 1.0000 loss_val: 6.9613 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0076 loss_train: 0.1915 acc_train: 1.0000 loss_val: 6.9732 acc_val: 0.0000 time: 0.9964s\n","Epoch: 0077 loss_train: 0.2005 acc_train: 1.0000 loss_val: 6.9821 acc_val: 0.0000 time: 1.0007s\n","Epoch: 0078 loss_train: 0.1813 acc_train: 1.0000 loss_val: 6.9931 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0079 loss_train: 0.1831 acc_train: 1.0000 loss_val: 7.0090 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0080 loss_train: 0.1742 acc_train: 1.0000 loss_val: 7.0271 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0081 loss_train: 0.1843 acc_train: 1.0000 loss_val: 7.0520 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0082 loss_train: 0.1704 acc_train: 1.0000 loss_val: 7.0818 acc_val: 0.0000 time: 0.9879s\n","Epoch: 0083 loss_train: 0.1523 acc_train: 1.0000 loss_val: 7.1130 acc_val: 0.0000 time: 0.9872s\n","Epoch: 0084 loss_train: 0.1452 acc_train: 1.0000 loss_val: 7.1460 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0085 loss_train: 0.1599 acc_train: 1.0000 loss_val: 7.1777 acc_val: 0.0000 time: 0.9867s\n","Epoch: 0086 loss_train: 0.1629 acc_train: 1.0000 loss_val: 7.2061 acc_val: 0.0000 time: 0.9874s\n","Epoch: 0087 loss_train: 0.1372 acc_train: 1.0000 loss_val: 7.2323 acc_val: 0.0000 time: 0.9874s\n","Epoch: 0088 loss_train: 0.1363 acc_train: 1.0000 loss_val: 7.2545 acc_val: 0.0000 time: 0.9867s\n","Epoch: 0089 loss_train: 0.1409 acc_train: 1.0000 loss_val: 7.2775 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0090 loss_train: 0.1356 acc_train: 1.0000 loss_val: 7.2950 acc_val: 0.0000 time: 0.9906s\n","Epoch: 0091 loss_train: 0.1211 acc_train: 1.0000 loss_val: 7.3098 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0092 loss_train: 0.1276 acc_train: 1.0000 loss_val: 7.3277 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0093 loss_train: 0.1263 acc_train: 1.0000 loss_val: 7.3412 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0094 loss_train: 0.1274 acc_train: 1.0000 loss_val: 7.3494 acc_val: 0.0000 time: 0.9879s\n","Epoch: 0095 loss_train: 0.1161 acc_train: 1.0000 loss_val: 7.3572 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0096 loss_train: 0.1145 acc_train: 1.0000 loss_val: 7.3639 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0097 loss_train: 0.1284 acc_train: 1.0000 loss_val: 7.3697 acc_val: 0.0000 time: 0.9911s\n","Epoch: 0098 loss_train: 0.1058 acc_train: 1.0000 loss_val: 7.3734 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0099 loss_train: 0.1175 acc_train: 1.0000 loss_val: 7.3801 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0100 loss_train: 0.1017 acc_train: 1.0000 loss_val: 7.3868 acc_val: 0.0000 time: 0.9874s\n","Epoch: 0101 loss_train: 0.1016 acc_train: 1.0000 loss_val: 7.3938 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0102 loss_train: 0.1133 acc_train: 1.0000 loss_val: 7.3976 acc_val: 0.0000 time: 0.9881s\n","Early Stopping...\n","Test set results: loss= 0.9492 accuracy= 0.7856\n","              precision    recall  f1-score   support\n","\n","           0     0.8542    0.9584    0.9033       746\n","           1     0.4444    0.2667    0.3333        15\n","           2     0.3636    0.4444    0.4000         9\n","           3     0.3333    0.3333    0.3333         3\n","           4     0.9375    0.7895    0.8571        19\n","           5     0.9677    0.8333    0.8955        36\n","           6     0.6667    0.6154    0.6400        13\n","           7     0.3750    0.8571    0.5217         7\n","           8     0.5789    0.5000    0.5366        22\n","          10     0.9464    0.4417    0.6023       120\n","          11     0.0000    0.0000    0.0000         1\n","          12     0.9717    0.9391    0.9551      1280\n","          13     0.0000    0.0000    0.0000         3\n","          14     1.0000    0.2000    0.3333         5\n","          15     0.2000    0.1304    0.1579        23\n","          16     0.8462    0.3793    0.5238        29\n","          17     0.0968    0.1875    0.1277        16\n","          18     0.0909    1.0000    0.1667         2\n","          19     1.0000    0.8000    0.8889         5\n","          20     0.1667    0.6667    0.2667         3\n","          21     0.0000    0.0000    0.0000         1\n","          22     0.4222    0.6477    0.5112        88\n","          23     0.1714    0.4615    0.2500        13\n","          24     0.2000    0.1667    0.1818        12\n","          26     0.5455    0.4000    0.4615        15\n","          27     0.0000    0.0000    0.0000         2\n","          28     0.0233    0.2500    0.0426         4\n","          29     0.2000    0.4000    0.2667         5\n","          30     0.0556    0.3333    0.0952         3\n","          31     0.0000    0.0000    0.0000         1\n","          32     0.4000    0.1250    0.1905        96\n","          33     0.5854    0.4898    0.5333        49\n","          34     0.1053    0.1818    0.1333        11\n","          35     0.0000    0.0000    0.0000         0\n","          36     1.0000    0.6667    0.8000         6\n","          37     0.0000    0.0000    0.0000         5\n","          39     0.0000    0.0000    0.0000         1\n","          40     0.2000    0.3333    0.2500        15\n","          41     0.3333    0.3333    0.3333         6\n","          42     0.8000    0.6667    0.7273        12\n","          43     0.4211    0.1739    0.2462        46\n","          44     0.0000    0.0000    0.0000         4\n","          45     0.9375    0.3846    0.5455        39\n","          46     0.0000    0.0000    0.0000         1\n","          47     0.1111    0.3750    0.1714         8\n","          48     0.8358    0.5283    0.6474       106\n","          49     0.3846    0.5556    0.4545         9\n","          50     1.0000    0.5714    0.7273         7\n","          51     0.0000    0.0000    0.0000         3\n","\n","    accuracy                         0.7856      2925\n","   macro avg     0.3994    0.3753    0.3472      2925\n","weighted avg     0.8272    0.7856    0.7914      2925\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0001 loss_train: 4.0946 acc_train: 0.0149 loss_val: 4.1725 acc_val: 0.0000 time: 0.9908s\n","Epoch: 0002 loss_train: 3.8210 acc_train: 0.0149 loss_val: 4.3951 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0003 loss_train: 3.5586 acc_train: 0.1642 loss_val: 4.6072 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0004 loss_train: 3.3161 acc_train: 0.2687 loss_val: 4.7169 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0005 loss_train: 3.0981 acc_train: 0.2836 loss_val: 4.8083 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0006 loss_train: 2.9567 acc_train: 0.2836 loss_val: 4.8707 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0007 loss_train: 2.8472 acc_train: 0.2687 loss_val: 4.9398 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0008 loss_train: 2.7582 acc_train: 0.2687 loss_val: 5.0087 acc_val: 0.0000 time: 0.9906s\n","Epoch: 0009 loss_train: 2.7409 acc_train: 0.2687 loss_val: 5.0701 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0010 loss_train: 2.6857 acc_train: 0.3134 loss_val: 5.1176 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0011 loss_train: 2.5535 acc_train: 0.2985 loss_val: 5.1535 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0012 loss_train: 2.4092 acc_train: 0.3433 loss_val: 5.1828 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0013 loss_train: 2.2876 acc_train: 0.4030 loss_val: 5.2091 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0014 loss_train: 2.1557 acc_train: 0.4179 loss_val: 5.2353 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0015 loss_train: 2.1529 acc_train: 0.4776 loss_val: 5.2631 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0016 loss_train: 2.0378 acc_train: 0.5522 loss_val: 5.2915 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0017 loss_train: 1.9551 acc_train: 0.5821 loss_val: 5.3213 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0018 loss_train: 1.8627 acc_train: 0.5970 loss_val: 5.3527 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0019 loss_train: 1.7895 acc_train: 0.7015 loss_val: 5.3857 acc_val: 0.0000 time: 0.9877s\n","Epoch: 0020 loss_train: 1.7834 acc_train: 0.6866 loss_val: 5.4195 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0021 loss_train: 1.6773 acc_train: 0.7313 loss_val: 5.4539 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0022 loss_train: 1.5678 acc_train: 0.7761 loss_val: 5.4904 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0023 loss_train: 1.4931 acc_train: 0.7463 loss_val: 5.5284 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0024 loss_train: 1.4354 acc_train: 0.8358 loss_val: 5.5673 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0025 loss_train: 1.3754 acc_train: 0.9104 loss_val: 5.6067 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0026 loss_train: 1.3517 acc_train: 0.8358 loss_val: 5.6442 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0027 loss_train: 1.2712 acc_train: 0.8657 loss_val: 5.6805 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0028 loss_train: 1.2425 acc_train: 0.8507 loss_val: 5.7155 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0029 loss_train: 1.1881 acc_train: 0.9104 loss_val: 5.7503 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0030 loss_train: 1.1163 acc_train: 0.9104 loss_val: 5.7867 acc_val: 0.0000 time: 0.9910s\n","Epoch: 0031 loss_train: 1.0790 acc_train: 0.8806 loss_val: 5.8235 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0032 loss_train: 1.0221 acc_train: 0.9254 loss_val: 5.8597 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0033 loss_train: 1.0290 acc_train: 0.8955 loss_val: 5.8954 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0034 loss_train: 0.9767 acc_train: 0.9254 loss_val: 5.9290 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0035 loss_train: 0.9460 acc_train: 0.8955 loss_val: 5.9612 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0036 loss_train: 0.8826 acc_train: 0.9104 loss_val: 5.9934 acc_val: 0.0000 time: 0.9876s\n","Epoch: 0037 loss_train: 0.8532 acc_train: 0.9254 loss_val: 6.0260 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0038 loss_train: 0.8021 acc_train: 0.9254 loss_val: 6.0614 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0039 loss_train: 0.8006 acc_train: 0.9552 loss_val: 6.0933 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0040 loss_train: 0.7951 acc_train: 0.9403 loss_val: 6.1226 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0041 loss_train: 0.7218 acc_train: 0.9552 loss_val: 6.1508 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0042 loss_train: 0.7098 acc_train: 0.9701 loss_val: 6.1783 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0043 loss_train: 0.6549 acc_train: 0.9403 loss_val: 6.2044 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0044 loss_train: 0.6250 acc_train: 0.9701 loss_val: 6.2325 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0045 loss_train: 0.6206 acc_train: 0.9403 loss_val: 6.2651 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0046 loss_train: 0.5878 acc_train: 0.9701 loss_val: 6.2988 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0047 loss_train: 0.5605 acc_train: 0.9552 loss_val: 6.3316 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0048 loss_train: 0.5554 acc_train: 0.9701 loss_val: 6.3655 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0049 loss_train: 0.5378 acc_train: 0.9701 loss_val: 6.4000 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0050 loss_train: 0.5156 acc_train: 0.9701 loss_val: 6.4363 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0051 loss_train: 0.4823 acc_train: 0.9552 loss_val: 6.4707 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0052 loss_train: 0.4703 acc_train: 0.9701 loss_val: 6.5051 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0053 loss_train: 0.4724 acc_train: 0.9552 loss_val: 6.5388 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0054 loss_train: 0.4465 acc_train: 0.9701 loss_val: 6.5738 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0055 loss_train: 0.4122 acc_train: 0.9701 loss_val: 6.6083 acc_val: 0.0000 time: 0.9875s\n","Epoch: 0056 loss_train: 0.4033 acc_train: 0.9851 loss_val: 6.6294 acc_val: 0.0000 time: 0.9872s\n","Epoch: 0057 loss_train: 0.3875 acc_train: 1.0000 loss_val: 6.6508 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0058 loss_train: 0.3786 acc_train: 0.9851 loss_val: 6.6767 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0059 loss_train: 0.3853 acc_train: 0.9851 loss_val: 6.7029 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0060 loss_train: 0.3427 acc_train: 1.0000 loss_val: 6.7345 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0061 loss_train: 0.3616 acc_train: 1.0000 loss_val: 6.7722 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0062 loss_train: 0.3169 acc_train: 1.0000 loss_val: 6.8083 acc_val: 0.0000 time: 0.9870s\n","Epoch: 0063 loss_train: 0.3093 acc_train: 1.0000 loss_val: 6.8429 acc_val: 0.0000 time: 0.9907s\n","Epoch: 0064 loss_train: 0.3208 acc_train: 1.0000 loss_val: 6.8712 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0065 loss_train: 0.3072 acc_train: 1.0000 loss_val: 6.8996 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0066 loss_train: 0.3021 acc_train: 0.9851 loss_val: 6.9191 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0067 loss_train: 0.3120 acc_train: 0.9851 loss_val: 6.9376 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0068 loss_train: 0.2692 acc_train: 0.9701 loss_val: 6.9517 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0069 loss_train: 0.2703 acc_train: 0.9851 loss_val: 6.9685 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0070 loss_train: 0.2724 acc_train: 1.0000 loss_val: 6.9946 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0071 loss_train: 0.2634 acc_train: 0.9851 loss_val: 7.0215 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0072 loss_train: 0.2552 acc_train: 0.9851 loss_val: 7.0466 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0073 loss_train: 0.2474 acc_train: 0.9851 loss_val: 7.0688 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0074 loss_train: 0.2354 acc_train: 0.9851 loss_val: 7.0890 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0075 loss_train: 0.2225 acc_train: 1.0000 loss_val: 7.1074 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0076 loss_train: 0.2349 acc_train: 1.0000 loss_val: 7.1245 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0077 loss_train: 0.2389 acc_train: 1.0000 loss_val: 7.1446 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0078 loss_train: 0.2173 acc_train: 1.0000 loss_val: 7.1713 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0079 loss_train: 0.2084 acc_train: 1.0000 loss_val: 7.2015 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0080 loss_train: 0.2020 acc_train: 1.0000 loss_val: 7.2325 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0081 loss_train: 0.1865 acc_train: 0.9851 loss_val: 7.2615 acc_val: 0.0000 time: 0.9922s\n","Epoch: 0082 loss_train: 0.1914 acc_train: 1.0000 loss_val: 7.2879 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0083 loss_train: 0.1897 acc_train: 1.0000 loss_val: 7.3133 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0084 loss_train: 0.1877 acc_train: 1.0000 loss_val: 7.3356 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0085 loss_train: 0.1637 acc_train: 1.0000 loss_val: 7.3476 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0086 loss_train: 0.1602 acc_train: 1.0000 loss_val: 7.3617 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0087 loss_train: 0.1667 acc_train: 1.0000 loss_val: 7.3773 acc_val: 0.0000 time: 0.9905s\n","Epoch: 0088 loss_train: 0.1663 acc_train: 1.0000 loss_val: 7.3896 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0089 loss_train: 0.1544 acc_train: 1.0000 loss_val: 7.4059 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0090 loss_train: 0.1611 acc_train: 1.0000 loss_val: 7.4220 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0091 loss_train: 0.1514 acc_train: 1.0000 loss_val: 7.4391 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0092 loss_train: 0.1527 acc_train: 1.0000 loss_val: 7.4603 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0093 loss_train: 0.1587 acc_train: 1.0000 loss_val: 7.4776 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0094 loss_train: 0.1484 acc_train: 1.0000 loss_val: 7.4957 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0095 loss_train: 0.1413 acc_train: 1.0000 loss_val: 7.5049 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0096 loss_train: 0.1332 acc_train: 1.0000 loss_val: 7.5078 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0097 loss_train: 0.1365 acc_train: 1.0000 loss_val: 7.5121 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0098 loss_train: 0.1443 acc_train: 1.0000 loss_val: 7.5146 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0099 loss_train: 0.1331 acc_train: 1.0000 loss_val: 7.5189 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0100 loss_train: 0.1119 acc_train: 1.0000 loss_val: 7.5220 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0101 loss_train: 0.1254 acc_train: 1.0000 loss_val: 7.5326 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0102 loss_train: 0.1291 acc_train: 0.9851 loss_val: 7.5473 acc_val: 0.0000 time: 0.9894s\n","Early Stopping...\n","Test set results: loss= 1.0088 accuracy= 0.7706\n","              precision    recall  f1-score   support\n","\n","           0     0.8587    0.9450    0.8998       746\n","           1     0.7500    0.2000    0.3158        15\n","           2     0.3636    0.8889    0.5161         9\n","           3     0.2500    0.3333    0.2857         3\n","           4     1.0000    0.7368    0.8485        19\n","           5     0.9677    0.8333    0.8955        36\n","           6     0.5714    0.6154    0.5926        13\n","           7     0.3158    0.8571    0.4615         7\n","           8     0.4286    0.1364    0.2069        22\n","          10     0.9483    0.4583    0.6180       120\n","          11     0.0000    0.0000    0.0000         1\n","          12     0.9671    0.9414    0.9541      1280\n","          13     0.0000    0.0000    0.0000         3\n","          14     1.0000    0.2000    0.3333         5\n","          15     0.1333    0.0870    0.1053        23\n","          16     0.8000    0.2759    0.4103        29\n","          17     0.1111    0.1250    0.1176        16\n","          18     0.2000    0.5000    0.2857         2\n","          19     0.8000    0.8000    0.8000         5\n","          20     0.2000    0.6667    0.3077         3\n","          21     0.0000    0.0000    0.0000         1\n","          22     0.4211    0.5455    0.4752        88\n","          23     0.2800    0.5385    0.3684        13\n","          24     0.2000    0.0833    0.1176        12\n","          26     0.5455    0.4000    0.4615        15\n","          27     0.0000    0.0000    0.0000         2\n","          28     0.0000    0.0000    0.0000         4\n","          29     0.2500    0.2000    0.2222         5\n","          30     0.0222    0.3333    0.0417         3\n","          31     0.0000    0.0000    0.0000         1\n","          32     0.3750    0.1250    0.1875        96\n","          33     0.5143    0.3673    0.4286        49\n","          34     0.0465    0.1818    0.0741        11\n","          35     0.0000    0.0000    0.0000         0\n","          36     1.0000    0.3333    0.5000         6\n","          37     0.0000    0.0000    0.0000         5\n","          39     0.0000    0.0000    0.0000         1\n","          40     0.1290    0.5333    0.2078        15\n","          41     0.2500    0.3333    0.2857         6\n","          42     0.6667    0.6667    0.6667        12\n","          43     0.4737    0.1957    0.2769        46\n","          44     0.0000    0.0000    0.0000         4\n","          45     0.8667    0.3333    0.4815        39\n","          46     0.0000    0.0000    0.0000         1\n","          47     0.0833    0.3750    0.1364         8\n","          48     0.9348    0.4057    0.5658       106\n","          49     0.2609    0.6667    0.3750         9\n","          50     0.4615    0.8571    0.6000         7\n","          51     0.0000    0.0000    0.0000         3\n","\n","    accuracy                         0.7706      2925\n","   macro avg     0.3765    0.3485    0.3148      2925\n","weighted avg     0.8243    0.7706    0.7781      2925\n","\n","Epoch: 0001 loss_train: 4.0752 acc_train: 0.0299 loss_val: 4.0906 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0002 loss_train: 3.7321 acc_train: 0.0448 loss_val: 4.2885 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0003 loss_train: 3.5037 acc_train: 0.1493 loss_val: 4.4750 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0004 loss_train: 3.2699 acc_train: 0.2239 loss_val: 4.6311 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0005 loss_train: 3.0638 acc_train: 0.2090 loss_val: 4.7206 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0006 loss_train: 2.9282 acc_train: 0.2836 loss_val: 4.7794 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0007 loss_train: 2.8042 acc_train: 0.3284 loss_val: 4.8337 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0008 loss_train: 2.7011 acc_train: 0.2985 loss_val: 4.8880 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0009 loss_train: 2.6368 acc_train: 0.3284 loss_val: 4.9365 acc_val: 0.0000 time: 0.9904s\n","Epoch: 0010 loss_train: 2.5446 acc_train: 0.3134 loss_val: 4.9778 acc_val: 0.0000 time: 0.9906s\n","Epoch: 0011 loss_train: 2.4648 acc_train: 0.3284 loss_val: 5.0106 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0012 loss_train: 2.3716 acc_train: 0.3433 loss_val: 5.0388 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0013 loss_train: 2.3022 acc_train: 0.3881 loss_val: 5.0655 acc_val: 0.0000 time: 0.9938s\n","Epoch: 0014 loss_train: 2.1454 acc_train: 0.4030 loss_val: 5.0925 acc_val: 0.0000 time: 0.9932s\n","Epoch: 0015 loss_train: 2.0811 acc_train: 0.4478 loss_val: 5.1207 acc_val: 0.0000 time: 0.9926s\n","Epoch: 0016 loss_train: 2.0276 acc_train: 0.5522 loss_val: 5.1501 acc_val: 0.0000 time: 0.9934s\n","Epoch: 0017 loss_train: 1.9453 acc_train: 0.5522 loss_val: 5.1801 acc_val: 0.0000 time: 0.9905s\n","Epoch: 0018 loss_train: 1.8476 acc_train: 0.6119 loss_val: 5.2097 acc_val: 0.0000 time: 0.9916s\n","Epoch: 0019 loss_train: 1.7911 acc_train: 0.6418 loss_val: 5.2388 acc_val: 0.0000 time: 0.9959s\n","Epoch: 0020 loss_train: 1.7173 acc_train: 0.7015 loss_val: 5.2673 acc_val: 0.0000 time: 0.9906s\n","Epoch: 0021 loss_train: 1.6329 acc_train: 0.7612 loss_val: 5.2960 acc_val: 0.0000 time: 0.9904s\n","Epoch: 0022 loss_train: 1.5795 acc_train: 0.8060 loss_val: 5.3244 acc_val: 0.0000 time: 0.9904s\n","Epoch: 0023 loss_train: 1.5196 acc_train: 0.8060 loss_val: 5.3531 acc_val: 0.0000 time: 0.9913s\n","Epoch: 0024 loss_train: 1.4507 acc_train: 0.8507 loss_val: 5.3834 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0025 loss_train: 1.3944 acc_train: 0.8358 loss_val: 5.4155 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0026 loss_train: 1.3224 acc_train: 0.8806 loss_val: 5.4496 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0027 loss_train: 1.2712 acc_train: 0.8806 loss_val: 5.4846 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0028 loss_train: 1.2244 acc_train: 0.8657 loss_val: 5.5198 acc_val: 0.0000 time: 0.9923s\n","Epoch: 0029 loss_train: 1.1985 acc_train: 0.8955 loss_val: 5.5558 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0030 loss_train: 1.1314 acc_train: 0.8955 loss_val: 5.5923 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0031 loss_train: 1.1025 acc_train: 0.8955 loss_val: 5.6289 acc_val: 0.0000 time: 0.9904s\n","Epoch: 0032 loss_train: 1.0353 acc_train: 0.9403 loss_val: 5.6653 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0033 loss_train: 0.9948 acc_train: 0.9104 loss_val: 5.7023 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0034 loss_train: 0.9862 acc_train: 0.8806 loss_val: 5.7397 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0035 loss_train: 0.9016 acc_train: 0.9104 loss_val: 5.7792 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0036 loss_train: 0.8718 acc_train: 0.9254 loss_val: 5.8182 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0037 loss_train: 0.8193 acc_train: 0.9552 loss_val: 5.8573 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0038 loss_train: 0.8282 acc_train: 0.9104 loss_val: 5.8962 acc_val: 0.0000 time: 0.9911s\n","Epoch: 0039 loss_train: 0.7390 acc_train: 0.9552 loss_val: 5.9380 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0040 loss_train: 0.7557 acc_train: 0.9254 loss_val: 5.9804 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0041 loss_train: 0.6880 acc_train: 0.9552 loss_val: 6.0260 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0042 loss_train: 0.6738 acc_train: 0.9552 loss_val: 6.0744 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0043 loss_train: 0.6738 acc_train: 0.9403 loss_val: 6.1211 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0044 loss_train: 0.6189 acc_train: 0.9851 loss_val: 6.1698 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0045 loss_train: 0.6293 acc_train: 0.9701 loss_val: 6.2122 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0046 loss_train: 0.5753 acc_train: 0.9552 loss_val: 6.2513 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0047 loss_train: 0.5559 acc_train: 0.9701 loss_val: 6.2846 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0048 loss_train: 0.5282 acc_train: 0.9851 loss_val: 6.3138 acc_val: 0.0000 time: 0.9918s\n","Epoch: 0049 loss_train: 0.5351 acc_train: 0.9851 loss_val: 6.3415 acc_val: 0.0000 time: 0.9924s\n","Epoch: 0050 loss_train: 0.5195 acc_train: 0.9701 loss_val: 6.3714 acc_val: 0.0000 time: 0.9904s\n","Epoch: 0051 loss_train: 0.4901 acc_train: 0.9701 loss_val: 6.3954 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0052 loss_train: 0.4527 acc_train: 1.0000 loss_val: 6.4208 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0053 loss_train: 0.4310 acc_train: 0.9851 loss_val: 6.4478 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0054 loss_train: 0.4243 acc_train: 0.9552 loss_val: 6.4794 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0055 loss_train: 0.4158 acc_train: 1.0000 loss_val: 6.5167 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0056 loss_train: 0.3845 acc_train: 0.9851 loss_val: 6.5556 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0057 loss_train: 0.4041 acc_train: 0.9851 loss_val: 6.5919 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0058 loss_train: 0.3871 acc_train: 0.9851 loss_val: 6.6318 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0059 loss_train: 0.3600 acc_train: 0.9851 loss_val: 6.6653 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0060 loss_train: 0.3356 acc_train: 1.0000 loss_val: 6.6939 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0061 loss_train: 0.3296 acc_train: 1.0000 loss_val: 6.7195 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0062 loss_train: 0.3035 acc_train: 0.9851 loss_val: 6.7458 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0063 loss_train: 0.3212 acc_train: 0.9851 loss_val: 6.7691 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0064 loss_train: 0.3097 acc_train: 1.0000 loss_val: 6.7969 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0065 loss_train: 0.2827 acc_train: 1.0000 loss_val: 6.8282 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0066 loss_train: 0.2785 acc_train: 0.9701 loss_val: 6.8607 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0067 loss_train: 0.2854 acc_train: 1.0000 loss_val: 6.8948 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0068 loss_train: 0.2678 acc_train: 1.0000 loss_val: 6.9330 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0069 loss_train: 0.2573 acc_train: 1.0000 loss_val: 6.9622 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0070 loss_train: 0.2515 acc_train: 0.9851 loss_val: 6.9959 acc_val: 0.0000 time: 0.9945s\n","Epoch: 0071 loss_train: 0.2430 acc_train: 0.9851 loss_val: 7.0257 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0072 loss_train: 0.2498 acc_train: 1.0000 loss_val: 7.0572 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0073 loss_train: 0.2342 acc_train: 1.0000 loss_val: 7.0888 acc_val: 0.0000 time: 0.9913s\n","Epoch: 0074 loss_train: 0.2217 acc_train: 1.0000 loss_val: 7.1174 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0075 loss_train: 0.2197 acc_train: 1.0000 loss_val: 7.1468 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0076 loss_train: 0.2066 acc_train: 1.0000 loss_val: 7.1722 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0077 loss_train: 0.2099 acc_train: 0.9851 loss_val: 7.1986 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0078 loss_train: 0.1962 acc_train: 1.0000 loss_val: 7.2231 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0079 loss_train: 0.1877 acc_train: 1.0000 loss_val: 7.2472 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0080 loss_train: 0.1937 acc_train: 1.0000 loss_val: 7.2753 acc_val: 0.0000 time: 0.9907s\n","Epoch: 0081 loss_train: 0.1891 acc_train: 1.0000 loss_val: 7.3044 acc_val: 0.0000 time: 0.9922s\n","Epoch: 0082 loss_train: 0.1819 acc_train: 1.0000 loss_val: 7.3308 acc_val: 0.0000 time: 0.9922s\n","Epoch: 0083 loss_train: 0.1966 acc_train: 0.9851 loss_val: 7.3593 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0084 loss_train: 0.1655 acc_train: 1.0000 loss_val: 7.3800 acc_val: 0.0000 time: 0.9905s\n","Epoch: 0085 loss_train: 0.1657 acc_train: 1.0000 loss_val: 7.3866 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0086 loss_train: 0.1710 acc_train: 1.0000 loss_val: 7.3850 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0087 loss_train: 0.1582 acc_train: 1.0000 loss_val: 7.3911 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0088 loss_train: 0.1543 acc_train: 1.0000 loss_val: 7.3960 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0089 loss_train: 0.1416 acc_train: 1.0000 loss_val: 7.4146 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0090 loss_train: 0.1384 acc_train: 1.0000 loss_val: 7.4418 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0091 loss_train: 0.1488 acc_train: 1.0000 loss_val: 7.4773 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0092 loss_train: 0.1591 acc_train: 1.0000 loss_val: 7.5189 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0093 loss_train: 0.1283 acc_train: 1.0000 loss_val: 7.5595 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0094 loss_train: 0.1338 acc_train: 1.0000 loss_val: 7.6026 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0095 loss_train: 0.1364 acc_train: 1.0000 loss_val: 7.6372 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0096 loss_train: 0.1294 acc_train: 1.0000 loss_val: 7.6752 acc_val: 0.0000 time: 0.9908s\n","Epoch: 0097 loss_train: 0.1362 acc_train: 1.0000 loss_val: 7.6978 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0098 loss_train: 0.1304 acc_train: 1.0000 loss_val: 7.7197 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0099 loss_train: 0.1277 acc_train: 1.0000 loss_val: 7.7345 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0100 loss_train: 0.1139 acc_train: 1.0000 loss_val: 7.7441 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0101 loss_train: 0.1134 acc_train: 1.0000 loss_val: 7.7452 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0102 loss_train: 0.1229 acc_train: 1.0000 loss_val: 7.7459 acc_val: 0.0000 time: 0.9893s\n","Early Stopping...\n","Test set results: loss= 1.0037 accuracy= 0.7874\n","              precision    recall  f1-score   support\n","\n","           0     0.8429    0.9638    0.8993       746\n","           1     0.5714    0.2667    0.3636        15\n","           2     0.3043    0.7778    0.4375         9\n","           3     0.2000    0.3333    0.2500         3\n","           4     0.9375    0.7895    0.8571        19\n","           5     0.9412    0.8889    0.9143        36\n","           6     0.5333    0.6154    0.5714        13\n","           7     0.3750    0.8571    0.5217         7\n","           8     0.5000    0.3636    0.4211        22\n","          10     0.9538    0.5167    0.6703       120\n","          11     0.0000    0.0000    0.0000         1\n","          12     0.9794    0.9289    0.9535      1280\n","          13     0.0000    0.0000    0.0000         3\n","          14     1.0000    0.2000    0.3333         5\n","          15     0.5000    0.1304    0.2069        23\n","          16     0.7692    0.3448    0.4762        29\n","          17     0.0714    0.1875    0.1034        16\n","          18     0.1667    0.5000    0.2500         2\n","          19     0.8000    0.8000    0.8000         5\n","          20     0.2000    0.6667    0.3077         3\n","          21     0.0000    0.0000    0.0000         1\n","          22     0.4138    0.6818    0.5150        88\n","          23     0.2857    0.4615    0.3529        13\n","          24     0.2500    0.0833    0.1250        12\n","          26     0.5000    0.6667    0.5714        15\n","          27     0.0000    0.0000    0.0000         2\n","          28     0.0000    0.0000    0.0000         4\n","          29     0.2000    0.2000    0.2000         5\n","          30     0.0667    0.3333    0.1111         3\n","          31     0.0000    0.0000    0.0000         1\n","          32     0.4483    0.1354    0.2080        96\n","          33     0.4444    0.3265    0.3765        49\n","          34     0.0278    0.0909    0.0426        11\n","          35     0.0000    0.0000    0.0000         0\n","          36     1.0000    0.5000    0.6667         6\n","          37     0.0000    0.0000    0.0000         5\n","          39     0.0000    0.0000    0.0000         1\n","          40     0.1489    0.4667    0.2258        15\n","          41     0.2500    0.3333    0.2857         6\n","          42     0.8000    0.6667    0.7273        12\n","          43     0.6364    0.1522    0.2456        46\n","          44     0.0000    0.0000    0.0000         4\n","          45     0.9333    0.3590    0.5185        39\n","          46     0.0000    0.0000    0.0000         1\n","          47     0.1250    0.3750    0.1875         8\n","          48     0.9275    0.6038    0.7314       106\n","          49     0.2400    0.6667    0.3529         9\n","          50     0.7143    0.7143    0.7143         7\n","          51     0.0000    0.0000    0.0000         3\n","\n","    accuracy                         0.7874      2925\n","   macro avg     0.3889    0.3663    0.3366      2925\n","weighted avg     0.8323    0.7874    0.7916      2925\n","\n","Epoch: 0001 loss_train: 3.9020 acc_train: 0.0448 loss_val: 4.2162 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0002 loss_train: 3.5264 acc_train: 0.1940 loss_val: 4.4915 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0003 loss_train: 3.3495 acc_train: 0.2537 loss_val: 4.7480 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0004 loss_train: 3.1490 acc_train: 0.2388 loss_val: 4.8466 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0005 loss_train: 2.9070 acc_train: 0.2687 loss_val: 4.9288 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0006 loss_train: 2.8792 acc_train: 0.3134 loss_val: 4.9848 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0007 loss_train: 2.7406 acc_train: 0.3134 loss_val: 5.0292 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0008 loss_train: 2.6849 acc_train: 0.3134 loss_val: 5.0651 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0009 loss_train: 2.5430 acc_train: 0.3582 loss_val: 5.0949 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0010 loss_train: 2.4583 acc_train: 0.3433 loss_val: 5.1189 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0011 loss_train: 2.3864 acc_train: 0.4179 loss_val: 5.1386 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0012 loss_train: 2.2638 acc_train: 0.4627 loss_val: 5.1566 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0013 loss_train: 2.1511 acc_train: 0.5075 loss_val: 5.1742 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0014 loss_train: 2.1022 acc_train: 0.5522 loss_val: 5.1931 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0015 loss_train: 1.9799 acc_train: 0.5821 loss_val: 5.2138 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0016 loss_train: 1.8919 acc_train: 0.6119 loss_val: 5.2364 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0017 loss_train: 1.8212 acc_train: 0.6866 loss_val: 5.2600 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0018 loss_train: 1.8010 acc_train: 0.6567 loss_val: 5.2846 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0019 loss_train: 1.6886 acc_train: 0.7910 loss_val: 5.3096 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0020 loss_train: 1.5997 acc_train: 0.7463 loss_val: 5.3353 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0021 loss_train: 1.5483 acc_train: 0.7463 loss_val: 5.3613 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0022 loss_train: 1.4955 acc_train: 0.8209 loss_val: 5.3886 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0023 loss_train: 1.4017 acc_train: 0.8358 loss_val: 5.4168 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0024 loss_train: 1.3953 acc_train: 0.8358 loss_val: 5.4457 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0025 loss_train: 1.3013 acc_train: 0.8507 loss_val: 5.4746 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0026 loss_train: 1.2176 acc_train: 0.8955 loss_val: 5.5054 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0027 loss_train: 1.2023 acc_train: 0.8806 loss_val: 5.5371 acc_val: 0.0000 time: 0.9913s\n","Epoch: 0028 loss_train: 1.1442 acc_train: 0.9104 loss_val: 5.5704 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0029 loss_train: 1.0980 acc_train: 0.8955 loss_val: 5.6050 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0030 loss_train: 1.0697 acc_train: 0.9254 loss_val: 5.6408 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0031 loss_train: 1.0025 acc_train: 0.8806 loss_val: 5.6761 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0032 loss_train: 0.9490 acc_train: 0.9104 loss_val: 5.7132 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0033 loss_train: 0.8703 acc_train: 0.9254 loss_val: 5.7525 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0034 loss_train: 0.9055 acc_train: 0.9254 loss_val: 5.7877 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0035 loss_train: 0.8283 acc_train: 0.9552 loss_val: 5.8231 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0036 loss_train: 0.7960 acc_train: 0.9254 loss_val: 5.8555 acc_val: 0.0000 time: 0.9944s\n","Epoch: 0037 loss_train: 0.8016 acc_train: 0.9552 loss_val: 5.8861 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0038 loss_train: 0.7404 acc_train: 0.9254 loss_val: 5.9175 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0039 loss_train: 0.7164 acc_train: 0.9403 loss_val: 5.9486 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0040 loss_train: 0.7032 acc_train: 0.9701 loss_val: 5.9786 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0041 loss_train: 0.6609 acc_train: 0.9552 loss_val: 6.0098 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0042 loss_train: 0.6240 acc_train: 0.9552 loss_val: 6.0435 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0043 loss_train: 0.5861 acc_train: 0.9701 loss_val: 6.0779 acc_val: 0.0000 time: 0.9883s\n","Epoch: 0044 loss_train: 0.5823 acc_train: 0.9851 loss_val: 6.1102 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0045 loss_train: 0.5588 acc_train: 0.9552 loss_val: 6.1422 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0046 loss_train: 0.5654 acc_train: 0.9701 loss_val: 6.1752 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0047 loss_train: 0.5095 acc_train: 0.9701 loss_val: 6.2076 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0048 loss_train: 0.5126 acc_train: 0.9851 loss_val: 6.2388 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0049 loss_train: 0.4791 acc_train: 0.9701 loss_val: 6.2766 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0050 loss_train: 0.4650 acc_train: 1.0000 loss_val: 6.3143 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0051 loss_train: 0.4920 acc_train: 0.9701 loss_val: 6.3508 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0052 loss_train: 0.4101 acc_train: 0.9701 loss_val: 6.3852 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0053 loss_train: 0.4084 acc_train: 0.9701 loss_val: 6.4188 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0054 loss_train: 0.3860 acc_train: 0.9552 loss_val: 6.4524 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0055 loss_train: 0.3820 acc_train: 1.0000 loss_val: 6.4837 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0056 loss_train: 0.3809 acc_train: 0.9851 loss_val: 6.5160 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0057 loss_train: 0.3595 acc_train: 1.0000 loss_val: 6.5452 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0058 loss_train: 0.3430 acc_train: 1.0000 loss_val: 6.5773 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0059 loss_train: 0.3355 acc_train: 1.0000 loss_val: 6.6125 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0060 loss_train: 0.3173 acc_train: 0.9851 loss_val: 6.6524 acc_val: 0.0000 time: 0.9881s\n","Epoch: 0061 loss_train: 0.3047 acc_train: 0.9851 loss_val: 6.6934 acc_val: 0.0000 time: 0.9879s\n","Epoch: 0062 loss_train: 0.2861 acc_train: 0.9851 loss_val: 6.7325 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0063 loss_train: 0.3011 acc_train: 1.0000 loss_val: 6.7734 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0064 loss_train: 0.2941 acc_train: 1.0000 loss_val: 6.8031 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0065 loss_train: 0.2753 acc_train: 1.0000 loss_val: 6.8319 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0066 loss_train: 0.2609 acc_train: 1.0000 loss_val: 6.8574 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0067 loss_train: 0.2555 acc_train: 1.0000 loss_val: 6.8746 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0068 loss_train: 0.2450 acc_train: 1.0000 loss_val: 6.8890 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0069 loss_train: 0.2391 acc_train: 0.9851 loss_val: 6.9100 acc_val: 0.0000 time: 0.9920s\n","Epoch: 0070 loss_train: 0.2234 acc_train: 1.0000 loss_val: 6.9297 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0071 loss_train: 0.2341 acc_train: 1.0000 loss_val: 6.9528 acc_val: 0.0000 time: 0.9910s\n","Epoch: 0072 loss_train: 0.2395 acc_train: 1.0000 loss_val: 6.9812 acc_val: 0.0000 time: 0.9910s\n","Epoch: 0073 loss_train: 0.2126 acc_train: 1.0000 loss_val: 7.0094 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0074 loss_train: 0.2328 acc_train: 1.0000 loss_val: 7.0331 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0075 loss_train: 0.2080 acc_train: 1.0000 loss_val: 7.0643 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0076 loss_train: 0.2051 acc_train: 1.0000 loss_val: 7.0967 acc_val: 0.0000 time: 0.9906s\n","Epoch: 0077 loss_train: 0.1886 acc_train: 1.0000 loss_val: 7.1322 acc_val: 0.0000 time: 0.9905s\n","Epoch: 0078 loss_train: 0.2153 acc_train: 1.0000 loss_val: 7.1653 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0079 loss_train: 0.1813 acc_train: 1.0000 loss_val: 7.1980 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0080 loss_train: 0.1748 acc_train: 1.0000 loss_val: 7.2294 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0081 loss_train: 0.1854 acc_train: 1.0000 loss_val: 7.2543 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0082 loss_train: 0.1658 acc_train: 1.0000 loss_val: 7.2771 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0083 loss_train: 0.1804 acc_train: 1.0000 loss_val: 7.2986 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0084 loss_train: 0.1599 acc_train: 1.0000 loss_val: 7.3194 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0085 loss_train: 0.1524 acc_train: 1.0000 loss_val: 7.3370 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0086 loss_train: 0.1464 acc_train: 1.0000 loss_val: 7.3526 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0087 loss_train: 0.1575 acc_train: 1.0000 loss_val: 7.3640 acc_val: 0.0000 time: 0.9948s\n","Epoch: 0088 loss_train: 0.1649 acc_train: 1.0000 loss_val: 7.3707 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0089 loss_train: 0.1574 acc_train: 1.0000 loss_val: 7.3798 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0090 loss_train: 0.1448 acc_train: 1.0000 loss_val: 7.3931 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0091 loss_train: 0.1438 acc_train: 1.0000 loss_val: 7.4060 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0092 loss_train: 0.1386 acc_train: 1.0000 loss_val: 7.4191 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0093 loss_train: 0.1372 acc_train: 1.0000 loss_val: 7.4328 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0094 loss_train: 0.1303 acc_train: 1.0000 loss_val: 7.4479 acc_val: 0.0000 time: 0.9903s\n","Epoch: 0095 loss_train: 0.1316 acc_train: 1.0000 loss_val: 7.4623 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0096 loss_train: 0.1243 acc_train: 1.0000 loss_val: 7.4824 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0097 loss_train: 0.1352 acc_train: 1.0000 loss_val: 7.4961 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0098 loss_train: 0.1297 acc_train: 1.0000 loss_val: 7.5173 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0099 loss_train: 0.1187 acc_train: 1.0000 loss_val: 7.5321 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0100 loss_train: 0.1296 acc_train: 1.0000 loss_val: 7.5446 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0101 loss_train: 0.1026 acc_train: 1.0000 loss_val: 7.5567 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0102 loss_train: 0.1107 acc_train: 1.0000 loss_val: 7.5715 acc_val: 0.0000 time: 0.9892s\n","Early Stopping...\n","Test set results: loss= 0.9812 accuracy= 0.7805\n","              precision    recall  f1-score   support\n","\n","           0     0.8602    0.9571    0.9061       746\n","           1     0.4000    0.2667    0.3200        15\n","           2     0.3158    0.6667    0.4286         9\n","           3     0.2500    0.3333    0.2857         3\n","           4     0.9286    0.6842    0.7879        19\n","           5     1.0000    0.8056    0.8923        36\n","           6     0.6154    0.6154    0.6154        13\n","           7     0.3158    0.8571    0.4615         7\n","           8     0.5556    0.4545    0.5000        22\n","          10     0.9028    0.5417    0.6771       120\n","          11     0.0000    0.0000    0.0000         1\n","          12     0.9749    0.9391    0.9566      1280\n","          13     0.0000    0.0000    0.0000         3\n","          14     1.0000    0.2000    0.3333         5\n","          15     0.2000    0.1304    0.1579        23\n","          16     0.9286    0.4483    0.6047        29\n","          17     0.0909    0.1250    0.1053        16\n","          18     0.1000    0.5000    0.1667         2\n","          19     1.0000    0.8000    0.8889         5\n","          20     0.2000    0.6667    0.3077         3\n","          21     0.0000    0.0000    0.0000         1\n","          22     0.4240    0.6023    0.4977        88\n","          23     0.2222    0.4615    0.3000        13\n","          24     0.2000    0.1667    0.1818        12\n","          26     0.3500    0.4667    0.4000        15\n","          27     0.0000    0.0000    0.0000         2\n","          28     0.0000    0.0000    0.0000         4\n","          29     0.3333    0.2000    0.2500         5\n","          30     0.0455    0.3333    0.0800         3\n","          31     0.0000    0.0000    0.0000         1\n","          32     0.3871    0.1250    0.1890        96\n","          33     0.5455    0.3673    0.4390        49\n","          34     0.0000    0.0000    0.0000        11\n","          35     0.0000    0.0000    0.0000         0\n","          36     0.6667    0.3333    0.4444         6\n","          37     0.0417    0.2000    0.0690         5\n","          39     0.0000    0.0000    0.0000         1\n","          40     0.1346    0.4667    0.2090        15\n","          41     0.3333    0.3333    0.3333         6\n","          42     0.8750    0.5833    0.7000        12\n","          43     0.5333    0.1739    0.2623        46\n","          44     0.0000    0.0000    0.0000         4\n","          45     0.8333    0.3846    0.5263        39\n","          46     0.0000    0.0000    0.0000         1\n","          47     0.1111    0.5000    0.1818         8\n","          48     0.9362    0.4151    0.5752       106\n","          49     0.4167    0.5556    0.4762         9\n","          50     0.8000    0.5714    0.6667         7\n","          51     0.0000    0.0000    0.0000         3\n","\n","    accuracy                         0.7805      2925\n","   macro avg     0.3842    0.3517    0.3301      2925\n","weighted avg     0.8294    0.7805    0.7891      2925\n","\n","Epoch: 0001 loss_train: 3.9626 acc_train: 0.0000 loss_val: 4.1437 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0002 loss_train: 3.6755 acc_train: 0.1791 loss_val: 4.3710 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0003 loss_train: 3.4133 acc_train: 0.2239 loss_val: 4.5902 acc_val: 0.0000 time: 0.9905s\n","Epoch: 0004 loss_train: 3.2103 acc_train: 0.2090 loss_val: 4.7186 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0005 loss_train: 3.0046 acc_train: 0.2836 loss_val: 4.7675 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0006 loss_train: 2.8469 acc_train: 0.3134 loss_val: 4.8208 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0007 loss_train: 2.7901 acc_train: 0.2985 loss_val: 4.8794 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0008 loss_train: 2.6412 acc_train: 0.3433 loss_val: 4.9358 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0009 loss_train: 2.5646 acc_train: 0.3582 loss_val: 4.9825 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0010 loss_train: 2.4793 acc_train: 0.3731 loss_val: 5.0185 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0011 loss_train: 2.3820 acc_train: 0.3582 loss_val: 5.0462 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0012 loss_train: 2.3219 acc_train: 0.4179 loss_val: 5.0681 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0013 loss_train: 2.1795 acc_train: 0.4478 loss_val: 5.0900 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0014 loss_train: 2.1037 acc_train: 0.5075 loss_val: 5.1139 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0015 loss_train: 2.0428 acc_train: 0.5075 loss_val: 5.1416 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0016 loss_train: 1.8950 acc_train: 0.5672 loss_val: 5.1713 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0017 loss_train: 1.8275 acc_train: 0.6119 loss_val: 5.2021 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0018 loss_train: 1.7646 acc_train: 0.6119 loss_val: 5.2333 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0019 loss_train: 1.6486 acc_train: 0.6866 loss_val: 5.2653 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0020 loss_train: 1.6174 acc_train: 0.7463 loss_val: 5.2963 acc_val: 0.0000 time: 0.9913s\n","Epoch: 0021 loss_train: 1.5526 acc_train: 0.7313 loss_val: 5.3272 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0022 loss_train: 1.5275 acc_train: 0.7761 loss_val: 5.3582 acc_val: 0.0000 time: 0.9934s\n","Epoch: 0023 loss_train: 1.4097 acc_train: 0.7910 loss_val: 5.3893 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0024 loss_train: 1.3985 acc_train: 0.8507 loss_val: 5.4197 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0025 loss_train: 1.3600 acc_train: 0.8209 loss_val: 5.4500 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0026 loss_train: 1.2862 acc_train: 0.8507 loss_val: 5.4809 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0027 loss_train: 1.1880 acc_train: 0.8209 loss_val: 5.5130 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0028 loss_train: 1.1652 acc_train: 0.8806 loss_val: 5.5466 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0029 loss_train: 1.1195 acc_train: 0.9254 loss_val: 5.5820 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0030 loss_train: 1.0772 acc_train: 0.8806 loss_val: 5.6186 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0031 loss_train: 1.0303 acc_train: 0.9104 loss_val: 5.6573 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0032 loss_train: 0.9736 acc_train: 0.8806 loss_val: 5.6950 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0033 loss_train: 0.9607 acc_train: 0.9403 loss_val: 5.7337 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0034 loss_train: 0.9010 acc_train: 0.9403 loss_val: 5.7712 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0035 loss_train: 0.8580 acc_train: 0.9552 loss_val: 5.8071 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0036 loss_train: 0.8287 acc_train: 0.9403 loss_val: 5.8415 acc_val: 0.0000 time: 0.9876s\n","Epoch: 0037 loss_train: 0.7889 acc_train: 0.9701 loss_val: 5.8769 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0038 loss_train: 0.7657 acc_train: 0.9403 loss_val: 5.9112 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0039 loss_train: 0.7391 acc_train: 0.9552 loss_val: 5.9448 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0040 loss_train: 0.7022 acc_train: 0.9552 loss_val: 5.9779 acc_val: 0.0000 time: 0.9878s\n","Epoch: 0041 loss_train: 0.6687 acc_train: 0.9552 loss_val: 6.0118 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0042 loss_train: 0.6460 acc_train: 0.9701 loss_val: 6.0482 acc_val: 0.0000 time: 0.9876s\n","Epoch: 0043 loss_train: 0.6132 acc_train: 0.9403 loss_val: 6.0852 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0044 loss_train: 0.5823 acc_train: 0.9701 loss_val: 6.1210 acc_val: 0.0000 time: 0.9878s\n","Epoch: 0045 loss_train: 0.5601 acc_train: 0.9851 loss_val: 6.1568 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0046 loss_train: 0.5484 acc_train: 0.9403 loss_val: 6.1897 acc_val: 0.0000 time: 0.9882s\n","Epoch: 0047 loss_train: 0.5306 acc_train: 0.9851 loss_val: 6.2229 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0048 loss_train: 0.5496 acc_train: 0.9701 loss_val: 6.2556 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0049 loss_train: 0.4915 acc_train: 0.9851 loss_val: 6.2885 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0050 loss_train: 0.4743 acc_train: 0.9851 loss_val: 6.3242 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0051 loss_train: 0.4486 acc_train: 1.0000 loss_val: 6.3606 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0052 loss_train: 0.4413 acc_train: 0.9701 loss_val: 6.3932 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0053 loss_train: 0.4275 acc_train: 0.9851 loss_val: 6.4233 acc_val: 0.0000 time: 0.9884s\n","Epoch: 0054 loss_train: 0.3951 acc_train: 0.9851 loss_val: 6.4558 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0055 loss_train: 0.3920 acc_train: 0.9701 loss_val: 6.4890 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0056 loss_train: 0.3702 acc_train: 0.9701 loss_val: 6.5204 acc_val: 0.0000 time: 0.9877s\n","Epoch: 0057 loss_train: 0.3640 acc_train: 1.0000 loss_val: 6.5502 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0058 loss_train: 0.3525 acc_train: 1.0000 loss_val: 6.5767 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0059 loss_train: 0.3378 acc_train: 1.0000 loss_val: 6.6065 acc_val: 0.0000 time: 0.9886s\n","Epoch: 0060 loss_train: 0.3312 acc_train: 0.9851 loss_val: 6.6353 acc_val: 0.0000 time: 0.9879s\n","Epoch: 0061 loss_train: 0.3102 acc_train: 1.0000 loss_val: 6.6662 acc_val: 0.0000 time: 0.9911s\n","Epoch: 0062 loss_train: 0.3061 acc_train: 1.0000 loss_val: 6.6980 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0063 loss_train: 0.3105 acc_train: 1.0000 loss_val: 6.7289 acc_val: 0.0000 time: 0.9888s\n","Epoch: 0064 loss_train: 0.2988 acc_train: 1.0000 loss_val: 6.7626 acc_val: 0.0000 time: 0.9905s\n","Epoch: 0065 loss_train: 0.2800 acc_train: 0.9701 loss_val: 6.7980 acc_val: 0.0000 time: 0.9916s\n","Epoch: 0066 loss_train: 0.2814 acc_train: 1.0000 loss_val: 6.8337 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0067 loss_train: 0.2682 acc_train: 1.0000 loss_val: 6.8663 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0068 loss_train: 0.2623 acc_train: 1.0000 loss_val: 6.8958 acc_val: 0.0000 time: 0.9899s\n","Epoch: 0069 loss_train: 0.2725 acc_train: 0.9851 loss_val: 6.9232 acc_val: 0.0000 time: 0.9905s\n","Epoch: 0070 loss_train: 0.2510 acc_train: 1.0000 loss_val: 6.9519 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0071 loss_train: 0.2331 acc_train: 1.0000 loss_val: 6.9772 acc_val: 0.0000 time: 0.9896s\n","Epoch: 0072 loss_train: 0.2235 acc_train: 1.0000 loss_val: 6.9921 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0073 loss_train: 0.2332 acc_train: 1.0000 loss_val: 7.0046 acc_val: 0.0000 time: 0.9902s\n","Epoch: 0074 loss_train: 0.2354 acc_train: 0.9851 loss_val: 7.0090 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0075 loss_train: 0.2145 acc_train: 1.0000 loss_val: 7.0177 acc_val: 0.0000 time: 0.9898s\n","Epoch: 0076 loss_train: 0.2132 acc_train: 1.0000 loss_val: 7.0312 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0077 loss_train: 0.2112 acc_train: 1.0000 loss_val: 7.0442 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0078 loss_train: 0.1955 acc_train: 1.0000 loss_val: 7.0593 acc_val: 0.0000 time: 0.9911s\n","Epoch: 0079 loss_train: 0.1897 acc_train: 1.0000 loss_val: 7.0810 acc_val: 0.0000 time: 0.9907s\n","Epoch: 0080 loss_train: 0.2060 acc_train: 1.0000 loss_val: 7.1076 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0081 loss_train: 0.1761 acc_train: 1.0000 loss_val: 7.1294 acc_val: 0.0000 time: 0.9900s\n","Epoch: 0082 loss_train: 0.1761 acc_train: 1.0000 loss_val: 7.1480 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0083 loss_train: 0.1920 acc_train: 1.0000 loss_val: 7.1665 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0084 loss_train: 0.1677 acc_train: 1.0000 loss_val: 7.1797 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0085 loss_train: 0.1626 acc_train: 1.0000 loss_val: 7.1938 acc_val: 0.0000 time: 0.9887s\n","Epoch: 0086 loss_train: 0.1543 acc_train: 1.0000 loss_val: 7.2088 acc_val: 0.0000 time: 0.9876s\n","Epoch: 0087 loss_train: 0.1465 acc_train: 1.0000 loss_val: 7.2234 acc_val: 0.0000 time: 0.9889s\n","Epoch: 0088 loss_train: 0.1480 acc_train: 1.0000 loss_val: 7.2454 acc_val: 0.0000 time: 0.9891s\n","Epoch: 0089 loss_train: 0.1548 acc_train: 1.0000 loss_val: 7.2806 acc_val: 0.0000 time: 0.9904s\n","Epoch: 0090 loss_train: 0.1596 acc_train: 1.0000 loss_val: 7.3103 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0091 loss_train: 0.1360 acc_train: 1.0000 loss_val: 7.3481 acc_val: 0.0000 time: 0.9897s\n","Epoch: 0092 loss_train: 0.1449 acc_train: 1.0000 loss_val: 7.3840 acc_val: 0.0000 time: 0.9893s\n","Epoch: 0093 loss_train: 0.1361 acc_train: 1.0000 loss_val: 7.4185 acc_val: 0.0000 time: 0.9894s\n","Epoch: 0094 loss_train: 0.1414 acc_train: 1.0000 loss_val: 7.4443 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0095 loss_train: 0.1267 acc_train: 1.0000 loss_val: 7.4615 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0096 loss_train: 0.1340 acc_train: 1.0000 loss_val: 7.4682 acc_val: 0.0000 time: 0.9895s\n","Epoch: 0097 loss_train: 0.1320 acc_train: 1.0000 loss_val: 7.4627 acc_val: 0.0000 time: 0.9885s\n","Epoch: 0098 loss_train: 0.1181 acc_train: 1.0000 loss_val: 7.4570 acc_val: 0.0000 time: 0.9890s\n","Epoch: 0099 loss_train: 0.1120 acc_train: 1.0000 loss_val: 7.4508 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0100 loss_train: 0.1180 acc_train: 1.0000 loss_val: 7.4487 acc_val: 0.0000 time: 0.9892s\n","Epoch: 0101 loss_train: 0.1123 acc_train: 1.0000 loss_val: 7.4527 acc_val: 0.0000 time: 0.9901s\n","Epoch: 0102 loss_train: 0.1070 acc_train: 1.0000 loss_val: 7.4637 acc_val: 0.0000 time: 0.9891s\n","Early Stopping...\n","Test set results: loss= 0.9861 accuracy= 0.7887\n","              precision    recall  f1-score   support\n","\n","           0     0.8547    0.9544    0.9018       746\n","           1     0.6667    0.2667    0.3810        15\n","           2     0.2857    0.8889    0.4324         9\n","           3     0.2500    0.3333    0.2857         3\n","           4     0.9333    0.7368    0.8235        19\n","           5     1.0000    0.8611    0.9254        36\n","           6     0.5000    0.6154    0.5517        13\n","           7     0.2000    0.8571    0.3243         7\n","           8     0.4444    0.3636    0.4000        22\n","          10     0.9839    0.5083    0.6703       120\n","          11     0.0000    0.0000    0.0000         1\n","          12     0.9747    0.9336    0.9537      1280\n","          13     0.0000    0.0000    0.0000         3\n","          14     1.0000    0.2000    0.3333         5\n","          15     0.2174    0.2174    0.2174        23\n","          16     0.9286    0.4483    0.6047        29\n","          17     0.0909    0.1250    0.1053        16\n","          18     0.2000    0.5000    0.2857         2\n","          19     1.0000    0.8000    0.8889         5\n","          20     0.2222    0.6667    0.3333         3\n","          21     0.0000    0.0000    0.0000         1\n","          22     0.4060    0.6136    0.4887        88\n","          23     0.3125    0.3846    0.3448        13\n","          24     0.3000    0.2500    0.2727        12\n","          26     0.5000    0.4667    0.4828        15\n","          27     0.0000    0.0000    0.0000         2\n","          28     0.0000    0.0000    0.0000         4\n","          29     0.0000    0.0000    0.0000         5\n","          30     0.0000    0.0000    0.0000         3\n","          31     0.0000    0.0000    0.0000         1\n","          32     0.4400    0.1146    0.1818        96\n","          33     0.6250    0.4082    0.4938        49\n","          34     0.0556    0.1818    0.0851        11\n","          35     0.0000    0.0000    0.0000         0\n","          36     1.0000    0.3333    0.5000         6\n","          37     0.0000    0.0000    0.0000         5\n","          39     0.0000    0.0000    0.0000         1\n","          40     0.1475    0.6000    0.2368        15\n","          41     0.4000    0.3333    0.3636         6\n","          42     0.8000    0.6667    0.7273        12\n","          43     0.4444    0.1739    0.2500        46\n","          44     0.0000    0.0000    0.0000         4\n","          45     0.8235    0.3590    0.5000        39\n","          46     0.0000    0.0000    0.0000         1\n","          47     0.1034    0.3750    0.1622         8\n","          48     0.8795    0.6887    0.7725       106\n","          49     0.3750    0.6667    0.4800         9\n","          50     0.6667    0.5714    0.6154         7\n","          51     0.0000    0.0000    0.0000         3\n","\n","    accuracy                         0.7887      2925\n","   macro avg     0.3884    0.3564    0.3342      2925\n","weighted avg     0.8314    0.7887    0.7947      2925\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-HQgfUgRZeS","executionInfo":{"status":"ok","timestamp":1612249744987,"user_tz":-660,"elapsed":910,"user":{"displayName":"Kunze Wang","photoUrl":"","userId":"02249298630517649291"}},"outputId":"9e2a65a6-8657-4d54-d870-507aa63dca9f"},"source":["print(np.round(final_acc_list,4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.7856 0.7706 0.7874 0.7805 0.7887]\n"],"name":"stdout"}]}]}